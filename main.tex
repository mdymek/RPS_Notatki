\documentclass[a4paper]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{lmodern}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{multirow}


\begin{document}
    \textbf{Prawdopodobieństwo warunkowe}
    \begin{equation}
        P(E|F) = \frac{P(E \cap F)}{P(F)}
    \end{equation}
    \begin{equation}
        P(E_1 \cap \dots \cap E_n) = \prod_{i=1}^{n} P(X_i | X_1, \dots, X_{i-1})
    \end{equation}
    \begin{equation}
        P(E) = \sum_{i=1}^{n} P(E|F_i)P(F_i) \text{ dla } \bigcap_{i=1}^{n} F_i = \Omega
    \end{equation}

    \begin{table}[H]
        \begin{center}
            \begin{tabular}{ p{5cm} | p{5cm} p{5.5cm} }
                & zmienne dyskretne & zmienne ciągłe\\
                \toprule
                definicja & $P(x) = P(X=x)$ & $f(x) = F'(x)$\\
                obliczanie prawdopodobieństwa & $P(X \in A) = \sum_{x \in A} P(x)$ & $P(X \in A) = \int_{A} f(x)dx$\\
                skumulowana f. rozkładu & $F(x) = P(X \leq x) = \sum_{y \leq x}P(y)$ &
                $F(x) = P(X \leq x) = \int_{-\infty}^{x}f(y)dy$\\
                całkowite prawdopodobieństwo & $\sum_{x}P(x) = 1$ & $\int_{- \infty}^{\infty}f(x)dx = 1$\\
                wartość oczekiwana & $EX = \sum_{x} x P(x)$ & $EX = \int xf(x) dx$\\
                wariancja & $VarX = \sigma^2 = E[ (X - \mu)^2 ]$ & $VarX = \int_{- \infty}^{\infty} (x - \mu)^2 f(x) dx$\\
                \bottomrule
            \end{tabular}
        \end{center}
    \end{table}

    \textbf{Nierówność Czebyszewa}
    \begin{equation}
        P( |X - \mu| > \epsilon ) \leq (\frac{\sigma}{\epsilon})^2
    \end{equation}

    \textbf{Rozkłady dyskretne}
    \begin{table}[H]
        \begin{center}
            \begin{tabular}{ p{2.3cm} | p{6cm} p{1.8cm} p{2.2cm} p{2.5cm}}
                Rozkład & P(x) & EX & VarX\\
                \toprule

                Bernoulli(p) &
                \[P(x) = \left\{\begin{array}{lr}
                                    p, & \text{for } x = 1\\
                                    q = (1-p), & \text{for } x = 0
                \end{array}\right.\]
                &
                $EX = p$
                &
                $VarX = pq$
                &
                próba\\
                \cmidrule(rl){2-5}

                Binomial(n, p) &
                $P(x) = \binom{n}{x} p^x (1-p)^{n-x} \text{ for } x = 0,1,\dots$
                &
                $EX = np$
                &
                $VarX = npq$
                &
                liczba sukcesów z n prób\\
                \cmidrule(rl){2-5}

                Geometric(p) &
                $P(x) = (1-p)^{x-1}p \text{ for } x = 1,2,\dots$

                $P(X>k) = (1-p)^k$
                &
                $EX = \frac{1}{p}$
                &
                $VarX = \frac{1-p}{p^2}$
                &
                liczba prób do sukcesu\\
                \cmidrule(rl){2-5}

                Poiss($\lambda$) &
                $P(x) = e^{-\lambda} \frac{\lambda^x}{x!} \text{ for } x = 0,1,\dots$
                &
                $EX = \lambda$
                &
                $VarX = \lambda$
                &
                rozkład zdarzeń rzadkich\\
                \bottomrule
            \end{tabular}
        \end{center}
    \end{table}


    \textbf{Rozkłady ciągłe}
    \begin{table}[H]
        \begin{center}
            \begin{tabular}{ p{2.3cm} | p{5.5cm} p{1.8cm} p{2.5cm} p{2.5cm}}
                Rozkład & f(x), F(x) & EX & VarX\\
                \toprule

                Unif(a,b) &
                $f(x) = \frac{1}{b-a} \text{ for } a \leq x \leq b$

                \[F(x) = \left\{\begin{array}{lr}
                                    0, & \text{for } x < a\\
                                    \frac{x-a}{b-a}, &  \text{for } a \leq x < b\\
                                    1, & \text{for } x \geq b
                \end{array}\right.\]
                &
                $EX = \frac{a+b}{2}$
                &
                $VarX = \frac{(b-a)^2}{12}$
                &
                \\
                \cmidrule(rl){2-5}

                Exp($\lambda$) &
                $f(x) = \lambda e^{-\lambda x} \text{ for } x \geq 0$

                $F(x) = 1 - e^{-\lambda x}$
                &
                $EX = \frac{1}{\lambda}$
                &
                $VarX = \frac{1}{\lambda^2}$
                & modelowanie czasu, brak pamięci\\
                \cmidrule(rl){2-5}

                Gamma($\alpha, \lambda$) &
                $f(x) = \frac{\lambda^{\alpha}}{\Gamma (\alpha)} x^{\alpha-1} e^{-\lambda x}$

                $F(x) = \frac{\lambda ^{\alpha}}{\Gamma(\alpha)} \int_{0}^{x} t^{\alpha-1} e^{-\lambda t} dt$
                &
                $EX = \frac{\alpha}{\lambda}$
                &
                $VarX = \frac{\alpha}{\lambda ^2}$
                &
                łączny czas $\alpha$ niezależnych zdarzeń $\sim Exp(\lambda)$\\
                \cmidrule(rl){2-5}

                N( $\mu, \sigma$) &
                $f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{\frac{-(x - \mu)^2}{2 \sigma^2}}$

                $F(x) = \Phi(x)$ dla N(0,1)
                &
                $EX = \mu$
                &
                $VarX = \sigma^2$
                &\\
                \bottomrule
            \end{tabular}
        \end{center}
    \end{table}

    \begin{equation}
        Bin(n, p) \approx Poiss(\lambda)
    \end{equation}

    \begin{equation}
        P(T \leq t) = P( X \geq \alpha)
    \end{equation}
    \begin{equation*}
        T \sim Gamma(\alpha, \lambda), X \sim Poiss(\lambda t)
    \end{equation*}

    \begin{equation}
        Binomial(n, p) = N(np, \sqrt{np(1-p)})
    \end{equation}
    \begin{equation*}
        X_i \sim Bernoulli(p), S_n = \sum_{i=1}^{n} X_i, 0.05 \leq p \leq -0.95
    \end{equation*}


    \textbf{Rozkład łączny}
    \begin{equation*}
        F_{(X,Y)}(x,y) = P( X \leq x \cap Y \leq y)
    \end{equation*}
    \begin{equation*}
        f_{(X,Y)}(x,y) = \frac{\delta^2}{\delta x \delta y} F_{(X,Y)}(x,y)
    \end{equation*}

    \begin{table}[H]
        \begin{center}
            \begin{tabular}{ p{5cm} | p{5.5cm} p{5cm} }
                & zmienne dyskretne & zmienne ciągłe\\
                \toprule
                rozkłady brzegowe &
                $P(x) = \sum_y P(x,y)$

                $P(y) = \sum_x P(x,y)$

                &
                $f(x) = \int_{Y} f(x,y)dy$

                $f(y) = \int_{X} f(x,y)dx$
                \\

                niezależność &
                $P(x,y) = P(x)P(y)$ &
                $f(x,y) = f(x)f(y)$
                \\

                obliczanie prawdopodobieństwa &
                $P( (X,Y) \in A) = \sum_{(x,y) \in A} P(x,y)$ &
                $\int \int_{(x,y) \in A} f(x,y)dx dy$\\

                \bottomrule
            \end{tabular}
        \end{center}
    \end{table}

    \textbf{Centralne Twierdzenie Graniczne}
    \begin{equation}
        Z_n = \frac{S_n - E(S_n)}{Std(S_n)} = \frac{S_n - n \mu}{\sqrt{n} \sigma} \rightarrow N(0,1) \text{ for } n \text{ to } \infty
    \end{equation}
    \begin{equation*}
        S_n = \sum_{i=1}^n X_i, E(X_i) = \mu, Std(X_i) = \sigma
    \end{equation*}

    \textbf{Estymatory - Monte Carlo}

    dla $X$, $p = P(X \in A)$
    \begin{equation*}
        \hat{p} = \hat{P}(X \in A) = \frac{\#(X_i \in A)}{n}
    \end{equation*}
    \begin{equation*}
        E(\hat{p}) = \frac{1}{n} (np) = p
    \end{equation*}
    \begin{equation*}
        Std(\hat{p}) = \frac{1}{n}\sqrt{np(1-p)} = \sqrt{\frac{p(1-p)}{n}}
    \end{equation*}
    Dokładność
    \begin{equation*}
        P(|\hat{p} - p| > \epsilon) = P \left ( \frac{|\hat{p} - p|}{\sqrt{\frac{p(1-p)}{n}}} > \frac{\epsilon}{\sqrt{\frac{p(1-p)}{n}}} \right )
        \approx 2 \Phi \left ( \frac{-\epsilon \sqrt{n}}{\sqrt{p(1-p)}} \right)
    \end{equation*}

    \textbf{Estymacja średniej} $\bar{X}$ z $X_1, \dots, X_n$ ze wspólnym $\mu$ i $\sigma$
    \begin{equation*}
        E \bar{X} = \frac{1}{n} (EX_1 + \dots + EX_n) = \frac{1}{n} n\mu = \mu
    \end{equation*}
    \begin{equation*}
        Var \bar{X} = \frac{1}{n^2} (Var(X_1) + \dots + Var(X_n)) = \frac{1}{n^2} n\sigma^2 = \frac{\sigma^2}{n}
    \end{equation*}

    \textbf{Estymator wariancji}
    \begin{equation*}
        s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2
    \end{equation*}

    \textbf{Procesy Markowa}
    \begin{equation*}
        P =
        \begin{bmatrix}
            p_{1 1} & p_{1 2} & \dots & p_{1 n}\\
            p_{2 1} & p_{2 2} & \dots & p_{2 n}\\
            \dots & \dots & \dots & \dots\\
            p_{n 1} & p_{n 2} & \dots & p_{n n}
        \end{bmatrix}
    \end{equation*}

    Rozkład w czasie h: $P_h = P_0 * P^h$

    Rozkład stacjonarny: $\pi P = \pi$, $\sum \pi_i = 1$

    Estymacja: \textbf{metoda momentów}

    \begin{table}[H]
        \begin{center}
            \begin{tabular}{ p{.4\textwidth} p{.6\textwidth}}
                k-ty moment z populacji &
                $\mu_k = E(X^k)$\\

                k-ty centralny moment z populacji &
                $\mu_k' = E((X - \mu_1)^k)$\\

                k-ty moment z próby &
                $m_k = \frac{1}{n} \sum_{i=1}^n X_i^k$\\

                k-ty centralny moment z próby &
                $m_k' = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^k$
            \end{tabular}
        \end{center}
    \end{table}

    $\mu_1 = EX, \mu_2' = VarX, \mu_k = m_k$\\

    Estymacja: \textbf{metoda największej wiarygodności}
    \begin{equation*}
        P(X = (X_1, \dots, X_n)) = P(X_1, \dots, X_n) = \prod_{i=1}^n P(X_i)
    \end{equation*}
    Szukamy ekstremum: $\frac{\delta P}{\delta \theta} (x) = 0$, używając logarytmu.


    \textbf{Przedziały ufności}
    \begin{equation*}
        \hat{\theta} \pm z_{\frac{\alpha}{2}} \sigma(\hat{\theta})
    \end{equation*}
    \begin{equation*}
        \bar{X} \pm z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}
    \end{equation*}
    \begin{equation*}
        \bar{X} - \bar{Y} \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma_x^2}{n_x} + \frac{\sigma_y^2}{n_y}}
    \end{equation*}

    \textbf{Rozkład t-studenta}

    $t = \frac{\hat{\theta} = \theta}{s(\hat{\theta})} \leftarrow$ zastępujemy $Std(\hat{\theta})$ przez $s(\hat{\theta})$,
    n-1 stopni swobody
    \begin{equation*}
        \bar{X} \pm t_{\frac{\alpha}{2}}^{(n-1)} \frac{s(\hat{\theta})}{\sqrt{n}}
    \end{equation*}

    \textbf{Z-testy}

    \begin{table}[H]
        \begin{center}
            \begin{tabular}{ p{2.5cm} |p{2.5cm} |p{1.5cm} |p{4cm} |p{4cm}}
                \toprule
                Hipoteza zerowa & Parametr,

                estymator & \multicolumn{2}{c|}{jeśli $H_0$ jest prawdziwa:} & Statystyka \\
                \toprule

                $H_0$ & $\theta, \hat{\theta}$ & $E(\hat{\theta})$ & $Var(\hat{\theta})$ & $Z = \frac{\hat{\theta} - \theta_0}{\sqrt{Var(\hat{\theta})}}$\\

                \cmidrule(r){1-5}

                $\mu = \mu_0$ & $\mu$, $\bar{X}$ & $\mu_0$ & $\frac{\sigma^2}{n}$ & $Z = \frac{\bar{X} - \mu_0}{\frac{\sigma}{\sqrt{n}}}$\\

                \cmidrule(r){1-5}

                $p = p_0$ & $p$, $\hat{p}$ & $p_0$ & $\frac{p_0(1-p_0)}{n}$ & $Z = \frac{\hat{p} - p_0}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}}$\\

                \cmidrule(r){1-5}

                $\mu_X - \mu_Y = D$ &
                $\mu_X - \mu_Y$,

                $\bar{X} - \bar{Y}$
                & $D$ & $\frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{n}$ & $Z = \frac{\bar{X} + \bar{Y} - D}{\sqrt{\frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{m}}}$\\

                \cmidrule(r){1-5}

                $p_1 - p_2 = D$ & $p_1 - p_2$,

                $\hat{p_1} - \hat{p_2}$
                & $D$ & $\frac{p_1(1-p_1)}{n} + \frac{p_2(1-p_2)}{m}$ &
                $Z = \frac{\hat{p_1} - \hat{p_2} - D}{\sqrt{\frac{\hat{p_1}(1-\hat{p_1})}{n} + \frac{\hat{p_2}(1-\hat{p_2})}{m}}}$\\

                \cmidrule(r){1-5}

                $p_1 = p_2$ & $p_1 - p_2$,

                $\hat{p_1} - \hat{p_2}$ & $0$ & $p(1-p)(\frac{1}{n} + \frac{1}{m})$

                gdzie $p = p_1 = p_2$ & $Z = \frac{\hat{p_1} - \hat{p_2}}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n} + \frac{1}{m})}}$

                gdzie $\hat{p} = \frac{n\hat{p_1} + m\hat{p_2}}{n + m}$\\
                \bottomrule
            \end{tabular}
        \end{center}
    \end{table}

    \textbf{T-testy}

    \begin{table}[H]
        \begin{center}
            \begin{tabular}{ p{.2\textwidth} |p{.3\textwidth} |p{.25\textwidth} |p{.25\textwidth}}
                \toprule
                Hipoteza zerowa & Warunki & Statystyka & Stopnie swobody\\
                \toprule

                $\mu = \mu_0$ & Rozmiar próby $n$;

                nieznana $\sigma$ & $t = \frac{\bar{X} - \mu_0}{\frac{s}{\sqrt{n}}}$ & $n-1$\\

                \cmidrule(r){1-4}

                $\mu_X - \mu_Y = D$ & Rozmiary prób $n$, $m$
                nieznane, równe $\sigma_X = \sigma_Y$ & $t = \frac{\bar{X} - \bar{Y} - D}{s_p\sqrt{\frac{1}{n}+\frac{1}{m}}}$
                & $n+m-2$\\

                \cmidrule(r){1-4}

                $\mu_X - \mu_Y = D$ & Rozmiary prób $n$, $m$;

                nieznane, różne $\sigma_X \neq \sigma_Y$ & $t = \frac{\bar{X} - \bar{Y} - D}{\sqrt{\frac{s_X^2}{n} + \frac{s_Y^2}{m}}}$
                & aproksymacja Satterthwaite\\

                \bottomrule
            \end{tabular}
        \end{center}
    \end{table}

    \textbf{P-wartości} - akceptacja $H_0$ jeśli $P \geq 0.1$, odrzucenie dla $P \leq 0.01$
    \begin{table}[H]
        \begin{center}
            \begin{tabular}{ p{.25\textwidth} |p{.25\textwidth} |p{.25\textwidth} |p{.25\textwidth}}
                \toprule
                Hipoteza zerowa & Hipoteza alternatywna & P-wartość & Wyliczenie\\
                \toprule
                \multirow{3}{*}{$\theta = \theta_0$} & prawostronna

                $\theta > \theta_0$ & $P\{Z \geq Z_{obs}\}$ & $1 - \Phi(Z_{obs})$\\

                & lewostronna

                $\theta < \theta_0$ & $P\{Z \leq Z_{obs}\}$ & $\Phi(Z_{obs})$\\

                & dwustronna

                $\theta \neq \theta_0$ & $P\{|Z| \geq |Z_{obs}|\}$ & $2(1 - \Phi(|Z_{obs}|))$\\

                \bottomrule
            \end{tabular}
        \end{center}
    \end{table}

    \textbf{Rozkład obserwacji o rozkładzie normalnym i wspólnej wariancji $\sigma^2$}
    \begin{equation*}
        \frac{(n-1)s^2}{\sigma^2} = \sum_{i=1}^{n} \left ( \frac{X_i - \bar{X}}{\sigma} \right )^2 \sim Chi-square(n-1) \sim Gamma \left ( \frac{n-1}{2}, \frac{1}{2} \right )
    \end{equation*}

    Zatem przedział ufności:
    \begin{equation*}
        \left [ \frac{(n-1)s^2}{\chi^2_{\frac{\alpha}{2}}}, \frac{(n-1)s^2}{\chi_{1 - \frac{\alpha}{2}}^2} \right ]
    \end{equation*}

    \textbf{Testy Chi kwadrat}
    \begin{table}[H]
        \begin{center}
            \begin{tabular}{ p{.1\textwidth} |p{.2\textwidth} |p{.2\textwidth} |p{.2\textwidth} | p{0.3\textwidth}}
                \toprule
                $H_0$ &  $H_A$ & Test statistic & Rejection region & P-value \\
                \toprule
                \multirow{3}{*}{$\sigma^2 = \sigma_0^2$} & $\sigma^2 > \sigma_0^2$
                & \multirow{3}{*}{$\frac{(n-1)s^2}{\sigma_0^2}$} & $\chi_{obs}^2 > \chi_{\alpha}^2$ & $P{\chi^2 \geq \chi_{obs}^2}$\\

                & $\sigma^2 < \sigma_0^2$ & & $\chi_{obs}^2 < \chi_{\alpha}^2$ & $P{\chi^2 \leq \chi_{obs}^2}$\\


                & $\sigma^2 \neq \sigma_0^2$ & & $\chi_{obs}^2 \geq \chi_{\frac{\alpha}{2}}^2$

                or $\chi_{obs}^2 \leq \chi_{\frac{\alpha}{2}}^2$
                & $2 min(P{\chi^2 \geq \chi_{obs}^2}, P{\chi^2 \leq \chi_{obs}^2})$\\

                \bottomrule
            \end{tabular}
        \end{center}
    \end{table}

    \textbf{Statystyka Chi-kwadrat}
    \begin{equation*}
        \chi^2 = \sum_{k=1}^{N} \frac{(Obs(k) - Exp(k))^2}{Exp(k)}, R = [\chi_{\alpha}^2, +\infty], P = P{\chi^2 \geq \chi_{obs}^2}
    \end{equation*}
    Rule of thumb: $Exp(k) \geq 5 \text{ for all }k = 1, \dots, N$.

    \textbf{Test Chi-kwadrat niezależności A i B}
    \begin{equation*}
        \chi_{obs}^2 = \sum_{i=1}^k \sum_{j=1}^m \frac{(Obs(i,j) - \hat{Exp}(i,j))^2}{\hat{Exp}(i,j)}, \hat{Exp}(i,j) = \frac{(n_{i.})(n_{.j})}{n}
    \end{equation*}


\end{document}